
<DOCTYPE html>
    <title>DisCo: Disentangled Implicit Content and Rhythm Learning for Diverse Co-Speech Gesture Synthesis</title>

    <meta charset="utf-8">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
    <link rel="stylesheet" href="css/style.css">
    <link rel="preconnect" href="https://fonts.gstatic.com">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="description" content="Project Page for Disco">
    <meta name="author" content="haiyang">

    <meta property="og:image" content="https://pantomatrix.github.io/DisCo/assets/teaser2.png">
    <meta property="og:url" content="https://pantomatrix.github.io/DisCo/">
    <meta property="og:type" content="website">
    <meta property="og:title" content="DisCo: Disentangled Implicit Content and Rhythm Learning for Diverse Co-Speech Gesture Synthesis">
    <meta property="og:video" content="https://www.youtube.com/embed/F6nXVTUY0KQ">
    <meta property="og:description" content="DisCo: Disentangled Implicit Content and Rhythm Learning for Diverse Co-Speech Gesture Synthesis.">


    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-JM2CPK6QLP"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-JM2CPK6QLP');
    </script>
</head>
    <style>
        /* greek-ext */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 300;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOlCnqEu92Fr1MmSU5fCBc4EsA.woff2) format('woff2');
            unicode-range: U+1F00-1FFF;
        }

        /* greek */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 300;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOlCnqEu92Fr1MmSU5fBxc4EsA.woff2) format('woff2');
            unicode-range: U+0370-03FF;
        }

        /* latin-ext */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 300;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOlCnqEu92Fr1MmSU5fChc4EsA.woff2) format('woff2');
            unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
        }

        /* latin */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 300;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOlCnqEu92Fr1MmSU5fBBc4.woff2) format('woff2');
            unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
        }

        /* greek-ext */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 400;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOmCnqEu92Fr1Mu7mxKOzY.woff2) format('woff2');
            unicode-range: U+1F00-1FFF;
        }

        /* greek */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 400;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOmCnqEu92Fr1Mu4WxKOzY.woff2) format('woff2');
            unicode-range: U+0370-03FF;
        }

        /* latin-ext */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 400;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOmCnqEu92Fr1Mu7GxKOzY.woff2) format('woff2');
            unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
        }

        /* latin */
        @font-face {
            font-family: 'Roboto';
            font-style: normal;
            font-weight: 400;
            font-display: swap;
            src: url(https://fonts.gstatic.com/s/roboto/v20/KFOmCnqEu92Fr1Mu4mxK.woff2) format('woff2');
            unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
        }
    </style>

    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <body>
        <div class="container">
            <div class="row mb-2 mt-4" id="paper-title">
                <h1 class="col-md-12 text-center">
                    DisCo
                </h1>
                <h3 class="col-md-12 text-center">
                    Disentangled Implicit Content and Rhythm Learning for Diverse Co-Speech Gesture Synthesis
                </h3>
                <h3 class="col-md-12 text-center">
                    <small>ACMMM 2022</small>
                </h3>

            </div>

            <div class="row" id="authors">
                <div class="mx-auto text-center">
                    <ul class="list-inline mb-0">
                        <li class="list-inline-item">
                            <a href="https://h-liu1997.github.io/">Haiyang Liu</a><sup>1</sup>
                        
                        <li class="list-inline-item">
                            <a href="https://iwanao731.github.io/">Naoya Iwamoto</a><sup>2</sup>

                        <li class="list-inline-item">
                            <a href="https://zzhat0706.github.io/PersonalPage/">Zihao Zhu</a><sup>3</sup>
                        
                        <li class="list-inline-item">
                            <a href="https://scholar.google.co.jp/citations?user=hgCoNowAAAAJ&hl=ja">Zhengqing Li</a><sup>2</sup>

                        <li class="list-inline-item">
                            <a href="">You Zhou</a><sup>3</sup>

                        <li class="list-inline-item">
                            <a href="https://sites.google.com/site/bozkurtelif/">Elif Bozkurt</a><sup>2</sup>

                        <li class="list-inline-item">
                            <a href="http://www.bozheng-lab.com/">Bo Zheng</a><sup>2</sup>
                    </ul>
                    <p id="institution">
                        <sup>1</sup>The University of Tokyo &nbsp;&nbsp;&nbsp;&nbsp;
                        <sup>2</sup>Digital Human Lab, HuaWei Technologies &nbsp;&nbsp;&nbsp;&nbsp;
                        <sup>3</sup>Keio University &nbsp;&nbsp;&nbsp;&nbsp;
                    </p>
                </div>
            </div>
            <div class="row mb-2" id="links">
                <div class="mx-auto">
                    <ul class="nav">
                        <li class="nav-item text-center">
                            <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548400" class="nav-link">
                                <svg style="width:48px;height:48px" viewBox="0 0 24 24">
                                    <path fill="currentColor" d="M16 0H8C6.9 0 6 .9 6 2V18C6 19.1 6.9 20 8 20H20C21.1 20 22 19.1 22 18V6L16 0M20 18H8V2H15V7H20V18M4 4V22H20V24H4C2.9 24 2 23.1 2 22V4H4M10 10V12H18V10H10M10 14V16H15V14H10Z" />
                                </svg><br>
                                Paper
                            </a>
                        </li>
                        <li class="nav-item text-center">
                            <a href="https://github.com/PantoMatrix/PantoMatrix/tree/main/scripts/DisCo_2022" class="nav-link">
                                <svg style="width:48px;height:48px" viewBox="0 0 24 24">
                                    <path fill="currentColor" d="M12,2A10,10 0 0,0 2,12C2,16.42 4.87,20.17 8.84,21.5C9.34,21.58 9.5,21.27 9.5,21C9.5,20.77 9.5,20.14 9.5,19.31C6.73,19.91 6.14,17.97 6.14,17.97C5.68,16.81 5.03,16.5 5.03,16.5C4.12,15.88 5.1,15.9 5.1,15.9C6.1,15.97 6.63,16.93 6.63,16.93C7.5,18.45 8.97,18 9.54,17.76C9.63,17.11 9.89,16.67 10.17,16.42C7.95,16.17 5.62,15.31 5.62,11.5C5.62,10.39 6,9.5 6.65,8.79C6.55,8.54 6.2,7.5 6.75,6.15C6.75,6.15 7.59,5.88 9.5,7.17C10.29,6.95 11.15,6.84 12,6.84C12.85,6.84 13.71,6.95 14.5,7.17C16.41,5.88 17.25,6.15 17.25,6.15C17.8,7.5 17.45,8.54 17.35,8.79C18,9.5 18.38,10.39 18.38,11.5C18.38,15.32 16.04,16.16 13.81,16.41C14.17,16.72 14.5,17.33 14.5,18.26C14.5,19.6 14.5,20.68 14.5,21C14.5,21.27 14.66,21.59 15.17,21.5C19.14,20.16 22,16.42 22,12A10,10 0 0,0 12,2Z" />
                                </svg><br>
                                Code
                            </a>
                        </li>
                    </ul>
                </div>
            </div>
            <div class="row mb-3 pt-2">
                <div class="col-md-8 mx-auto">
                    <div class="row no-gutters pb-2">
                        <div class="col-md-12">
                            <span></span><img src="assets/teaser.png" class="img-responsive">
                        </div>
                    </div>
                    <p class="text-justify">
                        DisCo is a framework for co-speech gestures synthesis in a disentangled manner, which generates more diverse gestures without sacrificing speech-motion synchrony. Left: the input speech (bottom), generated body and hands motion by S2G (top) and Ours (middle).
                        Right: T-SNE results for motion segments on Trinity test data. 
                    </p>

                </div>
            </div>
            <div class="row mb-3 pt-2">
                <div class="col-md-8 mx-auto">
                    <div class="row no-gutters pb-2">
                        <div class="col-md-12">
                            <span></span><img src="assets/pipeline.png" class="img-responsive">
                        </div>
                    </div>
                    <p class="text-justify">
                        We disentangles motion into implicit content and rhythm features by contrastive loss on both branches. Afterwards,
features are learned through different branches with different sampling strategies. a). Input speech is encoded and concatenated with the seed
motion. b). Rhythm features learning with original distribution. Contrastive loss disentangles features by pushing and pulling the distance
between negative and positive samples. c). Content branch first generates feature candidates through diversity networks, which consist
of two sub-network and adopt uniform and reverse sampling. Then selects the candidates are based on rhythm feature by the inclusion
network. d). Motion decoder takes rhythm and selected content features jointly with adversarial training to synthesize the final motion. 
                    </p>

                </div>
            </div>
            <div class="row mb-3 pt-2">
                <div class="col-md-8 mx-auto">
                    <div class="row no-gutters pb-2">
                        <div class="col-md-12">
                            <span></span><img src="assets/res.png" class="img-responsive">
                        </div>
                    </div>
                    <p class="text-justify">
                        Subjective Comparisons. Each sub-figure summarizes the 40s generation motion from Trinity dataset sequence 30, our method
generates more diverse gestures comparing with previous methods.
                    </p>

                </div>
            </div>
            <div class="row mb-4" id="overview-dataset">
                <div class="col-md-8 mx-auto grey-container">
                    <h4 class="pb-2"><strong>Paper Overview</strong></h4>
                    <div class="row pt-1 pb-1">
                        <div class="col-12">
                            <video loop="" playsinline="" controls="" width="85%">
                                <source src="assets/MM22_fp3051.mp4" type="video/mp4">
                            </video>
                        </div>
                    </div>
                    <br>
                    <p class="text-justify">
                        Current co-speech gestures synthesis methods struggle with generating diverse motions and typically collapse to single or few frequent motion sequences, which are trained on original data distribution with customized models and strategies. We tackle this problem by temporally clustering motion sequences into content and rhythm segments and then training on content-balanced data distribution. In particular, by clustering motion sequences, we have observed for each rhythm pattern, some motions appear frequently, while others appear less. This imbalance results in the difficulty of generating low frequent occurrence motions and it cannot be easily solved by resampling, due to the inherent many-to-many mapping between content and rhythm. Therefore, we present DisCo, which disentangles motion into implicit content and rhythm features by contrastive loss for adopting different data balance strategies. Besides, to model the inherent mapping between content and rhythm features, we design a diversity-and-inclusion network (DIN), which firstly generates content features candidates and then selects one candidate by learned voting. Experiments on two public datasets, Trinity and S2G-Ellen, justify that DisCo generates more realistic and diverse motions than state-of-the-art methods.
                    </p>
                </div>
            </div>
            <div class="row mb-2">
                <div class="col-md-8 mx-auto">
                    <h4 class="mb-3">Bibtex</h4>
                    <div class="bibtex">@inproceedings{liu2022disco,
                title={DisCo: Disentangled Implicit Content and Rhythm Learning for Diverse Co-Speech Gestures Synthesis},
                author={Liu, Haiyang and Iwamoto, Naoya and Zhu, Zihao and Li, Zhengqing and Zhou, You and Bozkurt, Elif and Zheng, Bo},
                booktitle={Proceedings of the 30th ACM International Conference on Multimedia},
                pages={3764--3773},
                year={2022}
            }
            </div>
                </div>
            </div>
            <div class="row mb-3">
                <div class="col-md-8 mx-auto">
                    <h4><strong>More Thanks</strong></h4>
                    <p class="text-justify">
                        The website is inspired by the template of <a href="https://alexyu.net/pixelnerf/" target="_blank">pixelnerf</a>.  Licensed under the Non-commercial license.
                    </p>
                </div>
            </div>
        </div> <!-- container -->
    </body>

</DOCTYPE>
